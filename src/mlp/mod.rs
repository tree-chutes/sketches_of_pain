// Copyright (c) 2026, tree-chutes

pub mod aggregator_functions;
mod conv2d;
pub mod layers;
mod linear_layer;
pub mod loss_functions;
mod mean_squares;
pub mod register;

#[cfg(test)]
mod tests {
    use crate::{mlp::{layers::{layer_factory, Layers}, loss_functions::{loss_function_factory, LossFunctions}}, read_payload, write_payload};

    #[test]
    fn test_full_pass() {
        let p_s = (0 as u16, 1 as u16);
        let mut bias: Vec<f32> = vec![];
        let input_layer = vec![ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.0, 48.0, 48.0, 22.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 97.0, 198.0, 243.0, 254.0, 254.0, 212.0, 
            27.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 67.0, 172.0, 254.0, 254.0, 
            225.0, 218.0, 218.0, 237.0, 248.0, 40.0, 0.0, 21.0, 164.0, 187.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 89.0, 219.0, 254.0, 97.0, 67.0, 14.0, 0.0, 0.0, 92.0, 231.0, 122.0, 23.0, 203.0, 236.0, 59.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0, 217.0, 242.0, 92.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0, 147.0, 253.0, 240.0, 232.0, 92.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 101.0, 255.0, 92.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 105.0, 254.0,
            254.0, 177.0, 11.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 167.0, 244.0, 41.0, 0.0, 0.0, 0.0, 
            7.0, 76.0, 199.0, 238.0, 239.0, 94.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 192.0, 
            121.0, 0.0, 0.0, 2.0, 63.0, 180.0, 254.0, 233.0, 126.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 190.0, 196.0, 14.0, 2.0, 97.0, 254.0, 252.0, 146.0, 52.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 130.0, 225.0, 71.0, 180.0, 232.0, 181.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 130.0, 254.0, 254.0, 230.0, 46.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 77.0, 244.0, 254.0, 162.0, 4.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 110.0, 254.0, 218.0, 254.0, 
            116.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 131.0, 
            254.0, 154.0, 28.0, 213.0, 86.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 66.0, 209.0, 153.0, 19.0, 19.0, 233.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 142.0, 254.0, 165.0, 0.0, 14.0, 216.0, 167.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 90.0, 254.0, 175.0, 0.0, 18.0, 229.0, 92.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 229.0, 249.0, 176.0, 222.0, 244.0, 44.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0, 193.0, 197.0, 134.0,
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
            0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0];

        let conv_0_weights = vec![0.9916829, 0.3851484, 0.4807757, 0.84955966, 0.89250755, 0.9445591, 0.18030953, 0.5798478, 
            0.7256626, 0.9034084, 0.05607915, 0.62737954, 0.23486304, 0.29393733, 0.43407142, 0.7913563, 0.15530431, 0.8031651, 0.04341781,
            0.6609131, 0.2686169, 0.30314374, 0.27999687, 0.04073322, 0.8395879, 0.7587124, 0.9797152, 0.984017, 0.6306827, 0.4571967, 
            0.60189617, 0.79433393, 0.16272652, 0.46091914, 0.99413025, 0.8396764, 0.4902097, 0.3466258, 0.401389, 0.61056125, 0.56846476, 
            0.45214367, 0.18605018, 0.93888044, 0.00057184696, 0.14951146, 0.8422059, 0.18620193, 0.20829892, 0.38251698, 0.0021859407, 
            0.5709597, 0.90630746, 0.70092213, 0.01113832, 0.44628847, 0.8567606, 0.9218352, 0.54330695, 0.13115287, 0.18426454, 0.24685264, 
            0.82451844, 0.6732143, 0.78556895, 0.33174074, 0.93075776, 0.8096006, 0.6018934, 0.17935383, 0.1371733, 0.55325985, 0.16458881, 
            0.78791547, 0.3058679, 0.5049561, 0.2393328, 0.47306097, 0.68102753, 0.7501395, 0.14247584, 0.7532463, 0.6454114, 0.07089925, 
            0.2198565, 0.2894913, 0.27204037, 0.74917734, 0.8858386, 0.013677716, 0.15586555, 0.061854362, 0.9545015, 0.99247, 0.68020487, 
            0.44306433, 0.84175146, 0.79821205, 0.39644432, 0.65879846, 0.48288155, 0.07894623, 0.8165251, 0.6706848, 0.8373585, 0.97396195, 
            0.15029585, 0.6390526, 0.34283006, 0.29697514, 0.84226024, 0.656759, 0.46073353, 0.6135397, 0.6531434, 0.42533648, 0.3523674, 
            0.822333, 0.16542196, 0.6447183, 0.900388, 0.72852397, 0.11487222, 0.80831504, 0.47585797, 0.5376961, 0.011704445, 0.08611643, 
            0.68478334, 0.011025786, 0.40545917, 0.46032798, 0.3976779, 0.050623894, 0.9933251, 0.54223573, 0.4525391, 0.39668608, 0.9107801, 
            0.041314363, 0.24075973, 0.27884114, 0.19564354, 0.5834198];

        let conv_1_weights = vec![0.283453, 0.05805564, 0.12175357, 0.90520275, 0.7199311, 0.8562279, 0.85638416, 0.20877194, 
            0.9650204, 0.25495982, 0.6126517, 0.15081835, 0.1802156, 0.5510732, 0.35302937, 0.5304885, 0.58718896, 0.4752519, 0.5134821, 
            0.26249433, 0.096101046, 0.45646036, 0.27791762, 0.26194954, 0.24083495, 0.012316704, 0.5459107, 0.77201176, 0.29557133, 0.013490438,
            0.52979136, 0.064451456, 0.6561985, 0.4718851, 0.41330004, 0.5090778, 0.92827857, 0.4955716, 0.27458787, 0.24194562, 0.75092876, 
            0.67367566, 0.050500512, 0.3395307, 0.97609544, 0.77781856, 0.27614903, 0.9377425, 0.74255717, 0.6956419, 0.537645, 0.35930383, 
            0.025520563, 0.2966168, 0.20705485, 0.71141136, 0.19765687, 0.2564125, 0.8797468, 0.6454896, 0.01109004, 0.965482, 0.46929824, 
            0.68825364, 0.09975624, 0.95533454, 0.8521967, 0.9764577, 0.32114565, 0.17616498, 0.27080393, 0.2052604, 0.27620912, 0.7068713, 
            0.3184836, 0.6838392, 0.02301991, 0.47310448, 0.12438917, 0.44723797, 0.61004794, 0.7093972, 0.98006094, 0.70874906, 0.30623114, 
            0.68586135, 0.4741993, 0.45365167, 0.348269, 0.41699708, 0.89819086, 0.09732878, 0.6496743, 0.03987825, 0.98861706, 0.5167904, 
            0.5233953, 0.76526, 0.580091, 0.5795008, 0.079476476, 0.69418216, 0.8747742, 0.60887897, 0.7850374, 0.7864225, 0.42668307, 0.8386766, 
            0.3236047, 0.53118265, 0.28619266, 0.13129187, 0.34787476, 0.13325655, 0.653841, 0.9108919, 0.31109047, 0.24951565, 0.9482869, 
            0.47740185, 0.5813463, 0.39782226, 0.6724944, 0.9798591, 0.24140799, 0.7926601, 0.67562723, 0.66813874, 0.90390265, 0.5146899, 
            0.5056175, 0.21496725, 0.60164785, 0.50072896, 0.7391212, 0.37391043, 0.3886794, 0.49534047, 0.8511653, 0.7517079, 0.83214045, 
            0.9063872, 0.30131578, 0.7748618];

        let linear_weights = vec![0.4148488, 0.31253898, 0.5259805, 0.6074592, 0.06927037, 0.99873376, 0.4431982, 0.1015743, 0.7989385, 
            0.5887047, 0.7894205, 0.0708276, 0.34424305, 0.9674443, 0.066094875, 0.9345591, 0.48126113, 0.81737936, 0.5804827, 0.031123519, 
            0.59788597, 0.8725141, 0.62375915, 0.08663511, 0.95910287, 0.6291779, 0.73071396, 0.4324504, 0.6467204, 0.07182169, 0.036253095, 
            0.390476, 0.7942294, 0.2068491, 0.68535113, 0.08448851];

        let pytorch_out: [f32; 289] = [ 335.9396,  530.1188,  694.0342,  938.3646, 1334.5043, 1735.4352,
        2171.9529, 2593.4551, 3019.2583, 3373.2542, 3331.4587, 3470.2258,
        3200.8831, 3046.4607, 2931.0933, 2861.2092, 2719.9358,  534.2770,
         660.5704,  920.2374, 1296.2164, 1719.5750, 2141.7190, 2471.0388,
        2633.9268, 3043.5261, 3665.1509, 3852.5110, 3996.6882, 3881.4197,
        3355.3323, 3335.0891, 3320.5737, 3367.9221,  651.1644,  755.3430,
        1241.1617, 1638.8838, 1951.0148, 2178.9045, 2267.6401, 2712.2415,
        3444.3418, 3993.7258, 4550.9155, 4818.0146, 4527.3193, 4295.1646,
        4160.3926, 4153.1875, 3561.2537,  767.6056,  939.9604, 1633.6285,
        1862.0341, 2033.9032, 2424.4614, 2779.7766, 3467.8398, 4216.8716,
        4693.8462, 5080.3525, 5294.0684, 5086.4414, 5185.7466, 4921.3213,
        3915.4907, 3359.7117,  877.2075, 1245.7867, 1901.4436, 2000.6768,
        2303.4883, 3023.1016, 3424.3989, 4166.5127, 4711.1387, 5183.0420,
        5743.9243, 5969.2256, 5420.6084, 4830.3828, 4395.9614, 4214.2134,
        4112.2109, 1034.0100, 1617.1051, 2207.7246, 2459.1599, 3000.7122,
        3499.6931, 3941.0732, 4777.9033, 5484.8604, 6029.5371, 6541.7515,
        5848.7285, 5094.7241, 4965.3101, 5009.5703, 5157.1104, 4514.3525,
        1231.6949, 1873.4379, 2665.0850, 2974.6836, 3452.8821, 3735.2756,
        4257.4619, 5315.7559, 6279.6807, 6498.4233, 6311.4639, 5580.6250,
        5429.3477, 5927.1113, 5639.5747, 4701.8784, 4087.8821, 1588.5406,
        2383.1179, 3215.8357, 3308.1807, 3662.3369, 4140.4023, 5078.6387,
        6090.7451, 6097.2700, 6238.4595, 6555.2144, 6446.1104, 5991.6885,
        5634.3608, 4844.3057, 4168.7046, 3831.4897, 1857.9948, 2993.0164,
        3431.9656, 3648.7605, 4163.1406, 4787.4902, 5205.2671, 5368.3037,
        5651.5044, 6348.2949, 6833.3311, 6389.9453, 5187.4263, 4398.9854,
        4032.9785, 3913.9209, 3529.3936, 2401.1050, 3258.5874, 3515.7615,
        4092.7380, 4631.7617, 4394.7095, 4415.2036, 5128.3022, 6034.9023,
        6144.2559, 5447.6123, 4684.0459, 3748.3718, 3287.9214, 2862.0234,
        2723.3025, 2449.9585, 2960.5781, 3591.4656, 4003.5093, 4445.2212,
        4178.7876, 4247.5562, 5238.4854, 5844.6060, 5867.3765, 5222.4468,
        4680.0630, 4315.5708, 3372.2480, 2560.9121, 2252.5310, 2166.0178,
        2118.2996, 3281.2026, 3701.3340, 3911.8333, 4171.1099, 4369.1621,
        5056.9556, 5498.0264, 5592.0601, 5392.8750, 4647.1367, 4236.7646,
        3932.1987, 2588.9731, 2079.9294, 1953.0044, 2032.1017, 1909.4766,
        3380.2188, 3631.3062, 4082.2253, 4984.4854, 5345.5522, 5423.3848,
        5311.8452, 5674.7363, 5641.4238, 4563.7324, 4124.5508, 3544.8606,
        2295.6484, 1955.0521, 1918.3485, 1759.3088, 1358.4744, 3204.0488,
        3810.5474, 4581.5913, 5074.4414, 4983.3306, 5190.5000, 5513.8716,
        5779.0933, 5331.1992, 4135.7539, 3897.2239, 3187.0071, 2133.6150,
        1749.6177, 1399.4298, 1000.0637,  670.4432, 3516.8491, 4159.3691,
        4580.8877, 4571.3047, 4870.9360, 5140.6895, 5148.9399, 5137.0679,
        4742.4238, 3741.3076, 3430.0325, 2777.0447, 1664.8663, 1021.4559,
         657.9788,  523.4617,  345.2279, 3548.1208, 3890.7383, 4198.9863,
        4462.3750, 4740.7290, 4577.4780, 4382.1294, 4544.9146, 4166.4155,
        3310.6406, 2959.6262, 2147.3218,  915.4572,  476.7493,  339.4326,
         202.6035,   59.5010, 3303.5457, 3960.2312, 3975.2295, 3815.4453,
        4175.5293, 3955.2874, 3934.9326, 4104.9810, 3757.4739, 2876.1738,
        2144.7891, 1372.9451,  609.4470,  246.7433,   45.6174,    0.0000,
        0.0000];

        let mut pytorch_out_conv1 = [279934.5000, 296306.5000, 306725.9688, 312847.2500, 313674.3438,
        311758.6562, 294014.0625, 307811.1250, 315320.3750, 317270.1562,
        314780.8750, 310827.1875, 305775.1562, 316824.1875, 320638.4375,
        319377.5625, 315557.1875, 310916.1875, 313388.5312, 319623.5000,
        320450.4062, 317042.6250, 312277.1562, 304201.8750, 315354.0000,
        319744.1875, 318688.1562, 311333.0938, 300999.8125, 289557.8125,
        313307.2188, 315324.0938, 309274.9375, 297447.5312, 285654.4062,
        271859.1562];

        let pytorch_out_17_k = [19459896., 25411994., 32581672., 40579984., 47393192., 51659256.,
        54246520., 56781124., 59836096., 61949604., 61991364., 58061872.,
        24782772., 32194734., 40301136., 48003720., 53966312., 57638000.,
        60226556., 63391712., 66553436., 67703496., 66326724., 60718416.,
        31487140., 39342836., 46820784., 53654544., 59669572., 64026000.,
        67218288., 70159672., 71997328., 71814152., 69617968., 62936292.,
        36995012., 44234476., 51345160., 58618836., 65645236., 70141888.,
        72560176., 74566568., 75506808., 74501688., 70601288., 61512132.,
        40985800., 48734636., 56489880., 64182488., 71029800., 74558632.,
        76263136., 77952624., 77787480., 74151368., 67332256., 57734644.,
        46444308., 55177636., 62817928., 69668568., 75608528., 78983976.,
        80731984., 80361592., 76436688., 70385408., 64201604., 56509936.,
        52364056., 60869784., 67506880., 73901888., 79591728., 82031384.,
        81579472., 78462920., 73385432., 68556312., 63206192., 54674140.,
        58261952., 66508520., 72937952., 78901008., 82767120., 82433224.,
        80526160., 78107952., 74127672., 68145088., 60280908., 50171800.,
        61484796., 68874312., 73338216., 76154024., 77583480., 77171896.,
        76402552., 74240720., 68621040., 60202196., 51424344., 42361096.,
        61485040., 66208128., 67902608., 68987976., 70258696., 70425008.,
        68552880., 64376308., 57148624., 48436172., 40805072., 32987802.,
        59561360., 62606404., 64327268., 65843932., 66113460., 64200568.,
        60652152., 55595728., 47955412., 39364892., 32415890., 25286204.,
        56963516., 60322096., 62229852., 62198988., 60267856., 56919104.,
        52862832., 47184196., 39245236., 31244838., 25118972., 19006700.];

        let pytorch_linear_36 = 5529637.5000;
        let pytorch_loss = 878381376.0;
        let pytorch_loss_grad = 59275.0;
        let pytorch_linear_updated_weights = [-165928368., -175632704., -181808736., -185437072., -185927328.,
        -184791808., -174273904., -182451952., -186902992., -188058704.,
        -186583216., -184239696., -181245168., -187794368., -190055216.,
        -189307856., -187043360., -184292448., -185757920., -189453632.,
        -189943760., -187923840., -185099152., -180312608., -186922928.,
        -189525168., -188899216., -184539584., -178414624., -171632496.,
        -185709712., -186905200., -183319632., -176309040., -169318784.,
        -161141792.];
        let pytorch_linear_updated_input = [24589.7480, 18525.4355, 31176.9668, 36006.5352,  4105.9321, 59198.9453,
        26270.1309,  6020.7153, 47356.2812, 34894.8828, 46792.1094,  4198.2354,
        20404.6621, 57344.2930,  3917.7075, 55395.0547, 28526.2734, 48449.3438,
        34407.5312,  1844.8154, 35439.0938, 51717.4023, 36972.6992,  5135.2095,
        56849.8633, 37293.8906, 43312.3398, 25633.0664, 38333.7070,  4257.1587,
         2148.8660, 23145.0742, 47077.1523, 12260.7734, 40623.5039,  5007.9722];

        let pytorch_conv_1_updated_weights = [-16057857., -19642976., -23761398., -28162208., -32970160., -38054128.,
        -42739076., -46034696., -47764176., -47833872., -46868328., -44794520.,
        -19058926., -23074164., -27474846., -32184748., -37587328., -42995572.,
        -47993912., -51408280., -52893936., -52726988., -51007548., -48578072.,
        -22308206., -26570516., -31544642., -36647712., -42463920., -48340076.,
        -53270172., -56242020., -57202024., -56464468., -54790264., -51567828.,
        -26058508., -30760764., -36058932., -41466700., -47288492., -53074884.,
        -57445968., -59935224., -60155316., -59107068., -56361892., -52384980.,
        -29806716., -34712068., -40067500., -45384472., -50870628., -55892288.,
        -59570768., -61358784., -60550240., -58238576., -54710724., -50466168.,
        -33464634., -38623864., -43600080., -48232544., -53333364., -57654664.,
        -60466324., -60914592., -58971952., -55496780., -51411316., -46534292.,
        -36963396., -41609804., -45923644., -50402256., -54803824., -58117360.,
        -59559436., -58764184., -55780876., -51501172., -46429848., -41506472.,
        -39912080., -44182956., -48252752., -52403176., -55566304., -57194600.,
        -57631664., -55714660., -51463132., -45702108., -40109920., -35418048.,
        -42437376., -46601808., -50175208., -53558504., -55665236., -56400972.,
        -55346896., -51683680., -46204612., -39891172., -34291272., -29738978.,
        -44243352., -47788480., -51010072., -53975744., -55192012., -54218140.,
        -51865904., -47176064., -41117868., -34039496., -28712982., -23897146.,
        -45402068., -48736696., -51714260., -53909220., -53771108., -51819484.,
        -48662512., -43494824., -36644968., -29632544., -24117194., -19383424.,
        -45664872., -48478212., -50850400., -52011288., -51137688., -48853912.,
        -44974104., -39151796., -32210778., -25149470., -19667166., -15012848.];

        let pytorch_conv_1_updated_input = [  6970.1548,   6678.7764,  12906.8184,  36531.0195,  41523.0273,
        84016.7734,  95897.1094,  84541.1641, 141671.8750, 107627.7109,
       111597.6797, 109310.2266,  47397.0078,  84938.0000,  23039.6953,
        36888.2109,   8928.4375,  11878.0039,  20121.3828,  41480.3477,
        80408.8750, 101274.2969, 142598.1875, 192187.6406, 194184.7969,
       208694.4219, 171757.4531, 172264.3438, 127521.6953, 130738.6406,
        70240.2031,  72596.6406,  27157.5996,  16140.5801,  16440.3965,
        37765.8828,  49211.6172, 127983.0312, 181402.0312, 228282.9219,
       260590.5625, 265567.0000, 302804.2500, 298128.2188, 202823.2656,
       278221.6875, 144900.2500, 203961.6719, 110209.3359,  73967.2109,
        38544.2969,  42583.7734,  55256.5078, 124548.1250, 183766.6250,
       226822.1094, 326453.0312, 371208.4375, 340692.8125, 361083.5312,
       320496.2188, 362864.1250, 296445.3750, 276961.9375, 249036.6875,
       196347.0000,  75414.8984,  71117.5625,  69875.6641,  96698.0000,
       157120.8438, 305077.4688, 368228.0000, 415081.0312, 426296.3438,
       454383.6875, 436179.1562, 402420.5625, 385949.4688, 397416.3750,
       378726.1875, 306385.4062, 208402.4688, 153819.2188,  68802.0391,
        57862.8359, 155640.5625, 211627.5781, 324945.7500, 406598.8125,
       507308.2812, 525147.5625, 480639.5938, 493500.0938, 481174.3125,
       512922.2188, 446146.4375, 465286.7812, 324846.4062, 272568.2500,
       132266.9688,  64779.8906,  68254.4922, 138507.9375, 209160.8750,
       386561.9375, 414893.3750, 476877.2500, 455841.5312, 443175.0625,
       500815.7500, 404185.0625, 497442.0938, 523484.7500, 426269.9375,
       362907.7500, 275784.2188, 202106.5000,  82388.4297,  93853.6562,
       156807.8438, 257796.8281, 324193.3438, 462305.5312, 479871.1250,
       496387.8125, 447537.3750, 484040.3438, 455527.9688, 550374.2500,
       569679.6250, 461514.2812, 442272.6562, 256786.9531, 205160.6719,
        53370.8633,  71142.6484, 201671.2812, 286720.5938, 366495.5938,
       441089.0000, 493754.4688, 516775.2812, 443612.2812, 517849.5625,
       489229.2188, 612267.6250, 569799.8750, 502334.5312, 436283.9688,
       344064.5312, 201685.7969,  94656.5938,  39686.0312, 172683.2812,
       259240.4062, 383321.2188, 415645.2500, 519020.0000, 486860.7188,
       523465.6562, 443366.1250, 561460.8750, 495651.0625, 606748.0625,
       462594.1250, 434276.4375, 285014.9375, 232662.6406,  64567.8867,
        59739.9453, 159067.4688, 234587.9375, 391601.3125, 409637.3438,
       521237.3125, 506744.2812, 508247.6875, 578901.7500, 586839.8750,
       672687.8750, 606768.6875, 514164.4375, 418220.1562, 335742.0000,
       166953.8125,  62063.4805,  72682.1641, 152427.4062, 267141.1562,
       369914.1875, 420235.4688, 547792.6250, 494305.3125, 535605.4375,
       524209.7812, 562658.4375, 633956.6250, 676218.3750, 566717.5000,
       434461.2500, 348265.1875, 183684.4531,  79960.8750,  69216.1484,
       148693.0000, 227531.8594, 341044.3438, 402613.1875, 420903.2812,
       477234.4375, 463201.9375, 513400.8438, 536725.2500, 588154.8750,
       537419.2500, 445682.6250, 379306.1250, 245146.3750, 150585.3438,
        22278.4590,  51801.5742, 115505.3906, 184325.2812, 270981.7812,
       287815.2188, 359906.6250, 354201.7188, 417360.0312, 400187.5312,
       434256.5000, 493521.8438, 453345.1250, 402928.7500, 279809.7500,
       209096.3750, 105793.0156,  44878.6484,  54446.9766,  71268.0859,
       154070.2188, 211890.8594, 237196.7812, 263539.7188, 260993.9375,
       261235.8125, 310363.1250, 330128.2812, 308769.5000, 318384.3750,
       250169.2344, 172476.2500, 128580.1562,  64732.8594,   7285.1655,
        35453.4336,  65215.4297, 124774.8047, 129460.0859, 187306.5156,
       180292.7344, 196237.1562, 223901.2188, 215044.8594, 252759.3906,
       236144.1094, 226322.0156, 175438.5625, 121329.0469,  61025.4688,
        42251.6250,   4375.3335,   1292.8823,  15001.4365,  41502.2500,
        48860.8867,  74866.7578,  60080.6797,  68710.5703,  68292.5312,
        82993.6406,  89102.3281, 107076.9844,  96313.1016,  80802.6875,
        81161.9766,  26280.5117,  32987.1367,   3880.5518];

        let pytorch_conv_0_updated_weights: [f32; 144] = [-207374528., -248341920., -286240736., -324510688., -369215232.,
        -415300544., -459301792., -488211808., -506043296., -506999904.,
        -490669664., -460729056., -240618928., -284363040., -324695808.,
        -366667360., -412561184., -461984864., -504966816., -533405664.,
        -545774528., -539467648., -516261440., -478501728., -277807168.,
        -326200096., -367822208., -408783040., -455324736., -506475616.,
        -545231296., -570221440., -576809152., -564425216., -534815200.,
        -489149088., -320328128., -368874624., -407296384., -447333856.,
        -493102656., -541941696., -573868224., -592279040., -592728576.,
        -572261056., -535679904., -483190880., -360922848., -407467744.,
        -439795456., -476962560., -514309472., -555657536., -582433984.,
        -594115136., -583155136., -554852544., -508805728., -453982176.,
        -397601504., -434598720., -462723904., -491604288., -525225120.,
        -557796672., -577716800., -575794368., -558118592., -517825696.,
        -466449504., -411806272., -423609856., -453825504., -478210560.,
        -502019744., -527120800., -551550656., -559241408., -547382208.,
        -516769952., -470808032., -419844864., -365629280., -448757568.,
        -474621344., -493723392., -509733216., -529390304., -540528448.,
        -536248032., -511170464., -469599552., -418793824., -367270752.,
        -315763360., -470678080., -494533888., -505655200., -519413600.,
        -524215776., -523611104., -504767840., -469492960., -417672832.,
        -362864288., -309887008., -263702784., -479576160., -497881440.,
        -508047520., -511627168., -508506720., -497825888., -471609824.,
        -423192544., -368789216., -310002400., -260063984., -213628144.,
        -478246496., -496299264., -501072864., -498850720., -489980512.,
        -472538336., -438296896., -387619424., -330297184., -268992768.,
        -219087872., -169088240., -465470976., -481439648., -484331936.,
        -478261472., -464098400., -442007616., -406319392., -352063072.,
        -289403584., -227753168., -175953184., -127831352.];

        let mut vectors = crate::Vectors::<f32> {
            input: vec![],
            conv_0_weights: vec![],
            conv_1_weights: vec![],
            linear_weights: vec![],
            target: vec![],
        };

        let mut lens = read_payload(&write_payload(
            &input_layer,
            &conv_0_weights,
            &conv_1_weights,
            &linear_weights,
            2.0,
        ), &mut vectors);

        let conv_layer_0 = layer_factory::<f32>(
            Layers::Conv2D,
            lens.0,
            lens.1,
            lens.0,
            Some(p_s),
            0.0,
        );
        let mut conv0_out = conv_layer_0.forward(( &vectors.input, &mut vectors.conv_0_weights, &bias) , None);
        let mut i = 0;
        while i < 17 * 17 {
            if (conv0_out[i] - pytorch_out[i]).abs()/pytorch_out[i] > 0.000001 {
                println!("{} {} {} {}", i, conv0_out[i], pytorch_out[i], conv0_out[i] - pytorch_out[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;
        }
        let output = (lens.0 + 2 * 0 - lens.1) / 1 + 1;
        let conv_layer_1 = layer_factory::<f32>(
            Layers::Conv2D,
            output,
            lens.2,
            output,
            Some(p_s),
            0.0,
        );

        let mut conv1_out = conv_layer_1.forward(( &conv0_out, &mut vectors.conv_1_weights, &bias) , None);
        i = 0;
        while i < 6 * 6 {
            if (conv1_out[i] - pytorch_out_conv1[i]).abs()/pytorch_out_conv1[i] > 0.000001{
                println!("{} {} {} {}", i, conv1_out[i], pytorch_out_conv1[i], conv1_out[i] - pytorch_out_conv1[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;

        }
        let linear_layer =
            layer_factory::<f32>(Layers::Linear, 1, lens.3, 1, Some(p_s), 0.0);
        let linear_out = linear_layer.forward((&conv1_out, &mut vectors.linear_weights, &bias), None);
        if (linear_out[0] - pytorch_linear_36).abs() / pytorch_linear_36 > 0.000001{
            println!("{} {} {}", linear_out[0], pytorch_linear_36, linear_out[0] - pytorch_linear_36);
            assert!(false, "error is bigger than 0.0001%")
        }
        let (flat_loss, squared) =
            loss_function_factory(LossFunctions::MeanSquares, vec![vec![5500000.0]], 1.0);
        let loss = squared.forward(&flat_loss, &linear_out);        
        if (loss[0] - pytorch_loss).abs() / pytorch_loss > 0.01{
            println!("{} {} {}%", loss[0], pytorch_loss, (loss[0] - pytorch_loss).abs()/pytorch_loss * 100.0);
            assert!(false, "error is bigger than 0.0001%")
        }
        if (linear_out[0] - pytorch_loss_grad).abs() / pytorch_loss_grad > 0.0001{
            println!("{} {} {}", linear_out[0] * 2.0 , pytorch_loss_grad, linear_out[0] * 2.0 - pytorch_loss_grad);
            assert!(false, "error is bigger than 0.0001%")
        }
        let learning_rate = 0.10;
        let updated = linear_layer.backward( (&mut conv1_out, &mut vectors.linear_weights, &mut bias) , learning_rate, &[linear_out[0]]);
        i = 0;
        while i < 6 * 6 {
            if (updated.0[i] - pytorch_linear_updated_input[i]).abs()/pytorch_linear_updated_input[i] > 0.000001{
                println!("{} {} {} {}", i, updated.0[i], pytorch_linear_updated_input[i], updated.1[i] - pytorch_linear_updated_input[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;
        }
        i = 0;
        while i < 6 * 6 {
            if (updated.1[i] - pytorch_linear_updated_weights[i]).abs()/pytorch_linear_updated_weights[i] > 0.000001{
                println!("{} {} {} {}", i, updated.1[i], pytorch_linear_updated_weights[i], updated.1[i] - pytorch_linear_updated_weights[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;

        }

        let updated = conv_layer_1.backward((&mut conv0_out, &mut vectors.conv_1_weights, &mut bias ), learning_rate, &updated.0);
        i = 0;
        while i < 17 * 17 {
            if (updated.0[i] - pytorch_conv_1_updated_input[i]).abs()/pytorch_conv_1_updated_input[i] > 0.0001{
                println!("{} {} {} {}", i, updated.0[i], pytorch_conv_1_updated_input[i], (updated.0[i] - pytorch_conv_1_updated_input[i]).abs()/pytorch_conv_1_updated_input[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;

        }
        i = 0;
        while i < 12 * 12 {
            if (updated.1[i] - pytorch_conv_1_updated_weights[i]).abs()/pytorch_conv_1_updated_weights[i] > 0.000001{
                println!("{} {} {} {}", i, updated.1[i], pytorch_conv_1_updated_weights[i], updated.1[i] - pytorch_conv_1_updated_weights[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;

        }
        let updated = conv_layer_0.backward((&mut vectors.input, &mut vectors.conv_0_weights, &mut bias ), learning_rate, &updated.0);
        i = 0;
        while i < 12 * 12 {
            if (updated.1[i] - pytorch_conv_0_updated_weights[i]).abs()/pytorch_conv_0_updated_weights[i] > 0.0001{
                println!("{} {} {} {}", i, updated.1[i], pytorch_conv_0_updated_weights[i], updated.1[i] - pytorch_conv_0_updated_weights[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;

        }

        vectors.input.clear();
        vectors.conv_0_weights.clear();
        vectors.conv_1_weights.clear();
        vectors.linear_weights.clear();

        lens = read_payload(&write_payload(
            &input_layer,
            &pytorch_out,
            &conv_1_weights,
            &linear_weights,
            2.0,
        ), &mut vectors);

        let conv_layer_1 = layer_factory::<f32>(
            Layers::Conv2D,
            lens.0,
            lens.1,
            lens.0,
            Some(p_s),
            0.0,
        );
        let out = conv_layer_1.forward(( &vectors.input, &mut vectors.conv_0_weights, &bias) , None);
        let mut i = 0;
        while i < 12 * 12 {
            if (out[i] - pytorch_out_17_k[i]).abs()/pytorch_out_17_k[i]  > 0.000001 {
                println!("{} {} {} {}", i, out[i], pytorch_out_17_k[i], out[i] - pytorch_out_17_k[i]);
                assert!(false, "error is bigger than 0.0001%")
            }
            i += 1;
        }
    }
}
